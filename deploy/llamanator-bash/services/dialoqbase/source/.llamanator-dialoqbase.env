# You can get your API key from https://platform.openai.com/account/api-keys
# If you have Azure openai key please refer here https://js.langchain.com/docs/getting-started/guide-llm
OPENAI_API_KEY=""
# DB_SECRET_KEY is used for jwt token generation please change it to your own secret key
DB_SECRET_KEY="super-secret-key"
# Cohere API key  -> https://dashboard.cohere.ai/api-keys
COHERE_API_KEY=""
# Huggingface Hub API key -> https://huggingface.co/settings/token
HUGGINGFACEHUB_API_KEY=""
# Anthropic API key -> https://earlyaccess.anthropic.com/
ANTHROPIC_API_KEY=""
# Github personal access token -> https://github.com/settings/tokens/new
GITHUB_ACCESS_TOKEN=""
# Google PaLM API key -> https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=""
# Eleven labs API Key -> https://elevenlabs.io/
ELEVENLABS_API_KEY=""
# Dialoqbase Q Concurency
DB_QUEUE_CONCURRENCY=1
# Dialoqbase Session Secret, it should be 32 characters long
DB_SESSION_SECRET=""
# Dialoqbase Session Secure
DB_SESSION_SECURE="false"
# Jina API key -> https://jina.ai/embeddings/
JINA_API_KEY=""# Enable/Disable Services

# If you are running Ollama on a remote server, you can skip installing it on the Llamanator server.
ENABLE_OLLAMACPU=true # If you don't have a GPU, set this to true
ENABLE_OLLAMAGPU=false # If you have a GPU, set this to true and set Ollama CPU to false
ENABLE_OLLAMA_BASE_MODELS=true # If you want to automatically download llama2, llama3, mistral, nomic-embed and codellama models, set this to true. This will take some time to download.
ENABLE_OPENWEBUI=true
ENABLE_DIALOQBASE=true

# General Variables
SERVER_IP=5.161.110.136 #Enter the IP address of your server
DOMAIN_NAME=mjdev1.dsodemo.org #OPTIONAL

## -----Inferencing Services----- ##
# OLLAMA Variables
## If you are deploying Ollama as part of the Llamanator project, you don't need to change these variables. If you are hosting it elsewhere, update the address accordingly.
OLLAMACPU_COMPOSE_FILE=./services/ollama/source/docker-compose-cpu.yml
OLLAMAGPU_COMPOSE_FILE=./services/ollama/source/docker-compose-gpu.yml
# Only change the below if you are using a remote Ollama server and not the one provided by Llamanator
OLLAMA_ENDPOINT=http://5.161.110.136:11434
OLLAMACPU_PORT=11434
OLLAMAGPU_PORT=11434

# Llamanator Chat Services
## OpenWebUI Variables
OPENWEBUI_COMPOSE_FILE=./services/open-webui/source/docker-compose-llamanator.yml
OPENWEBUI_PORT=10000

## LibreChat Variables
LIBRECHAT_COMPOSE_FILE=./services/librechat/source/docker-compose-llamanator.yml
LIBRECHAT_PORT=10001

# Llamanator RAG Services
## Danswer Variables
DANSWER_COMPOSE_FILE=./services/danswer/source/docker-compose-llamanator.yml
DANSWER_PORT=10010

## DialoqBase Variables
DIALOQBASE_COMPOSE_FILE=./services/dialoqbase/source/docker-compose-llamanator.yml
DIALOQBASE_PORT=10011

# Llamanator AI App Build Services
## Flowise Variables
FLOWISE_COMPOSE_FILE=./services/flowise/source/docker-compose-llamanator.yml
FLOWISE_PORT=10020

## LangFlow Variables
LANGFLOW_COMPOSE_FILE=./services/langflow/source/docker-compose-llamanator.yml
LANGFLOW_PORT=10021

# Llamantor AI/ML Model Training Services
## Jupyter Variables
JUPYTER_COMPOSE_FILE=./services/jupyter/source/docker-compose-llamanator.yml
JUPYTER_PORT=10030

## H2OLMStudio Variables
H2OLMSTUDIO_COMPOSE_FILE=./services/h2o-lm-studio/source/docker-compose-llamanator.yml
H2OLMSTUDIO_PORT=10031

## VSCode Variables
VSCODE_COMPOSE_FILE=./services/vscode-server/source/docker-compose-llamanator.yml
VSCODE_PORT=10040
VSCODE_PASSWORD=LockItDown2024
VSCODE_TIMEZONE=America/New_York

# Llamanator Variables - DO NOT CHANGE
HAPROXY_COMPOSE_FILE=./services/llamanator/haproxy/docker-compose.yml
HAPROXY_PATH=./services/llamanator/haproxy
CONFIG_TEMPLATE=/haproxy.cfg.template
CONFIG_OUTPUT=/haproxy.cfg
LINKS_TEMPLATE=./templates/llamanator-links.txt.template
LINKS_OUTPUT=./llamanator-links.txt